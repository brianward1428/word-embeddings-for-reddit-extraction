{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mounted-showcase",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianward/Desktop/Northeastern/AI/project/aiEnv/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "from myFunctions import processSentences, meanVector, cosineDistance\n",
    "\n",
    "import statistics\n",
    "\n",
    "\n",
    "\n",
    "# NLP Packages\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-greek",
   "metadata": {},
   "source": [
    "# data import\n",
    "\n",
    "Okay so Kaggle has two pretty awesome wallstreet bets datasets.  \n",
    "One only has the title but has a LOT of posts,  \n",
    "and the other has a body and titles.  \n",
    "Im going to only use the body from the second source and use all the titles from the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afraid-exhaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianward/Desktop/Northeastern/AI/project/aiEnv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (5,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "reddit = pd.read_csv(\"data/reddit_wsb.csv\")\n",
    "reddit2 = pd.read_csv(\"data/r_wallstreetbets_posts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "patent-campbell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length reddit : 43057\n",
      "legnth reddit without NA : 20678\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's not about the money, it's about sending a...</td>\n",
       "      <td>55</td>\n",
       "      <td>l6ulcx</td>\n",
       "      <td>https://v.redd.it/6j75regs72e61</td>\n",
       "      <td>6</td>\n",
       "      <td>1.611863e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:37:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score      id  \\\n",
       "0  It's not about the money, it's about sending a...     55  l6ulcx   \n",
       "\n",
       "                               url  comms_num       created body  \\\n",
       "0  https://v.redd.it/6j75regs72e61          6  1.611863e+09  NaN   \n",
       "\n",
       "             timestamp  \n",
       "0  2021-01-28 21:37:41  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('length reddit :', len(reddit))\n",
    "print('legnth reddit without NA :', len(reddit.dropna()))\n",
    "\n",
    "reddit.head(1)\n",
    "# SO what do we actually want to train the model on?\n",
    "# we can use the titles and we can use the bodys. \n",
    "# we also dont really need to store any of this other information for model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "light-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we parse each body into sentences split on '.' probably...\n",
    "# okay so lets go ahead and do this first, we can also treat each title as a its own sentence.\n",
    "\n",
    "sentences_raw = []\n",
    "\n",
    "\n",
    "for i in range(len(reddit)):\n",
    "    body = reddit['body'][i]\n",
    "    if isinstance(body, str):\n",
    "        sentences_raw.extend(body.split('.'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "solar-rabbit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280855"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many sentences do we have now?\n",
    "\n",
    "len(sentences_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-funeral",
   "metadata": {},
   "source": [
    "# second datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "designed-vintage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161920"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddit2)+len(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pleased-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay so now were going to add these titles to our sentences.\n",
    "\n",
    "for i in range(len(reddit2)):\n",
    "    title = reddit2['title'][i]\n",
    "    if isinstance(title, str):\n",
    "        sentences_raw.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stable-sandwich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of raw sentences :  1399717\n"
     ]
    }
   ],
   "source": [
    "print('length of raw sentences : ', len(sentences_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-satin",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "Okay so now we need to process these strings to prep them for the model training.   \n",
    "What we will do :  \n",
    "* string cleaning\n",
    "* tokenizing sentences\n",
    "* creating bigrams\n",
    "* Should we remove stopwords?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-morocco",
   "metadata": {},
   "source": [
    "# String Cleaning \n",
    "* lowercase \n",
    "* remove punctuation\n",
    "* remove numbers \n",
    "* clean whitespace\n",
    "* tokenize\n",
    "* find bigrams\n",
    "* find trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "still-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply our string cleaning. \n",
    "sentences_clean = processSentences(sentences_raw, minStringSize = 5, minTokenCount = 3, splitonPeriod = True, phraseMinCount = 25, bigrams=True, trigrams=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "periodic-learning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'ceo',\n",
       " 'of',\n",
       " 'nasdaq',\n",
       " 'pushed',\n",
       " 'to',\n",
       " 'halt_trading',\n",
       " '“to',\n",
       " 'give',\n",
       " 'investors',\n",
       " 'a',\n",
       " 'chance',\n",
       " 'to',\n",
       " 'recalibrate',\n",
       " 'their',\n",
       " 'positions”']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets take a look at where we're at:\n",
    "sentences_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vanilla-throw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so we have 1205382 sentences to train our model on\n"
     ]
    }
   ],
   "source": [
    "print('so we have {} sentences to train our model on'.format(len(sentences_clean)))\n",
    "# damn thats alot hell yea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "latest-triple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“these',\n",
       " 'small',\n",
       " 'and',\n",
       " 'unsophisticated',\n",
       " 'investors',\n",
       " 'are',\n",
       " 'probably',\n",
       " 'going',\n",
       " 'to',\n",
       " 'get',\n",
       " 'hurt',\n",
       " 'by',\n",
       " 'this']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_clean[501]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-taste",
   "metadata": {},
   "source": [
    "# Import Text8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "continuous-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "text8 = api.load('text8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-blank",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with text8 as a base. \n",
    "model = Word2Vec(text8, min_count=25, window =2, sg = 1, vector_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "brave-pressure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98296687, 126667930)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can then continue to train on our model on the sentences we have created.\n",
    "model.build_vocab(sentences_clean, update=True)\n",
    "model.train(sentences_clean,total_examples=len(sentences_clean), epochs = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eleven-administration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amc', 0.8408816456794739),\n",
       " ('pltr', 0.8126963973045349),\n",
       " ('bb', 0.8063815832138062),\n",
       " ('gamestop', 0.7710800766944885),\n",
       " ('nok', 0.7422802448272705),\n",
       " ('gma', 0.7160327434539795),\n",
       " ('gamestonk', 0.7137332558631897),\n",
       " ('game_stop', 0.7121042609214783),\n",
       " ('nakd', 0.6815323829650879),\n",
       " ('gmc', 0.6784548759460449)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['gme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-lexington",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sophisticated-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we dont need to save the whole model (only for further training) instead we will save the wordvectors\n",
    "\n",
    "# model.wv.save('V2.wordvectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "public-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test:\n",
    "\n",
    "# another way to test this is to do it by set agian...\n",
    "\n",
    "def testV2(keyedVectors, myVec, groundTruth, threshold):\n",
    "    \n",
    "    vocab = set()\n",
    "    expected = set()\n",
    "    \n",
    "    for i in range(len(groundTruth)):\n",
    "        \n",
    "        for word in groundTruth['sent'][i].split(' '):\n",
    "            if word in keyedVectors:\n",
    "                vocab.add(word)\n",
    "        \n",
    "        for word2 in groundTruth['expected'][i]:\n",
    "            if word2 in keyedVectors:\n",
    "                expected.add(word2)\n",
    "\n",
    "    \n",
    "    # okay so calculate missed:    \n",
    "    missed = set()\n",
    "    \n",
    "    for word in expected:\n",
    "        if cosineDistance(myVec, keyedVectors[word]) < threshold :\n",
    "            missed.add(word)\n",
    "    \n",
    "    # first need to subtract the expected set.\n",
    "    notExpected = vocab - expected\n",
    "    \n",
    "    over = set()\n",
    "    \n",
    "    for word in notExpected:\n",
    "        if cosineDistance(myVec, keyedVectors[word]) >= threshold :\n",
    "            over.add(word)\n",
    "    missedScore = (len(expected)-len(missed))/len(expected)\n",
    "    overScore = (len(notExpected)-len(over))/len(notExpected)\n",
    "    \n",
    "    return [missedScore, overScore, statistics.mean([missedScore, overScore])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "republican-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundTruth = pd.read_csv(\"data/groundTruth.csv\")\n",
    "groundTruth = groundTruth[['sent','expected']]\n",
    "\n",
    "def tokenize(sent):\n",
    "    if type(sent) == str:\n",
    "        return sent.strip().split(' ')\n",
    "    else: \n",
    "        return []\n",
    "\n",
    "    \n",
    "# apply to our expected column\n",
    "groundTruth['expected'] = groundTruth['expected'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "amino-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "myVec = meanVector(model.wv, positive=['gme', 'amc','game_stop','tsla', 'nok', 'etsy', 'crox', 'appl', 'microsoft', 'facebook', 'netflix', 'dropbox','slack', 'peloton', 'wal_mart', 'lululemon'], \n",
    "                negative=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "asian-still",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6842105263157895, 0.979890310786106, 0.8320504185509477]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testV2(model.wv, myVec, groundTruth, 0.55)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
